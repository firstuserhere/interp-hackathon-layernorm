{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7b_TeB7aYn3"
      },
      "source": [
        "**SOURCE NOTEBOOK:** [TransformerLens Exploratory Analysis Demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i4YiFbQaYn4"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGJjYo4DaYn4",
        "outputId": "d627380e-97fc-43d7-9963-dab34da58389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Colab notebook\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-ev0s_lsm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-ev0s_lsm\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 218ebd6f491f47f5e2f64e4c4327548b60a093eb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.13.1)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.6.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.2.20)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.25.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.4.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.65.0)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.30.2)\n",
            "Collecting typeguard<4.0.0,>=3.0.2 (from transformer-lens==0.0.0)\n",
            "  Using cached typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.15.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.27.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2022.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer-lens==0.0.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer-lens==0.0.0) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.3.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.4)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.32)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.28.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n",
            "Installing collected packages: typeguard\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.13.3\n",
            "    Uninstalling typeguard-2.13.3:\n",
            "      Successfully uninstalled typeguard-2.13.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pysvelte 1.0.0 requires typeguard~=2.0, but you have typeguard 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typeguard-3.0.2\n",
            "\n",
            "## Installing the NodeSource Node.js 16.x repo...\n",
            "\n",
            "\n",
            "## Populating apt-get cache...\n",
            "\n",
            "+ apt-get update\n",
            "Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:2 https://deb.nodesource.com/node_16.x focal InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "\n",
            "## Confirming \"focal\" is supported...\n",
            "\n",
            "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/focal/Release'\n",
            "\n",
            "## Adding the NodeSource signing key to your keyring...\n",
            "\n",
            "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n",
            "\n",
            "## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n",
            "\n",
            "+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x focal main' > /etc/apt/sources.list.d/nodesource.list\n",
            "+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x focal main' >> /etc/apt/sources.list.d/nodesource.list\n",
            "\n",
            "## Running `apt-get update` for you...\n",
            "\n",
            "+ apt-get update\n",
            "Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:2 https://deb.nodesource.com/node_16.x focal InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "\n",
            "## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n",
            "## You may also need development tools to build native addons:\n",
            "     sudo apt-get install gcc g++ make\n",
            "## To install the Yarn package manager, run:\n",
            "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n",
            "     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
            "     sudo apt-get update && sudo apt-get install yarn\n",
            "\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "nodejs is already the newest version (16.20.1-deb-1nodesource1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Collecting git+https://github.com/neelnanda-io/PySvelte.git\n",
            "  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-9ei51niz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-9ei51niz\n",
            "  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 6f5d971a148d40fb7481d400ae74551b37340e83\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.25.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.30.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.5.3)\n",
            "Collecting typeguard~=2.0 (from PySvelte==1.0.0)\n",
            "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2.27.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->PySvelte==1.0.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->PySvelte==1.0.0) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->PySvelte==1.0.0) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->PySvelte==1.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->PySvelte==1.0.0) (1.3.0)\n",
            "Installing collected packages: typeguard\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 3.0.2\n",
            "    Uninstalling typeguard-3.0.2:\n",
            "      Successfully uninstalled typeguard-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformer-lens 0.0.0 requires typeguard<4.0.0,>=3.0.2, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEBUG_MODE = False\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "    # Install another version of node that makes PySvelte work way faster\n",
        "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the TransformerLens code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps00qBQBaYn7"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEBUG_MODE:\n",
        "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2vfqL90aYn7"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOii7yRaaYn7"
      },
      "outputs": [],
      "source": [
        "import pysvelte\n",
        "\n",
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-XBeyeXaYn8"
      },
      "source": [
        "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkBlooINaYn8"
      },
      "outputs": [],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEBXP6yraYn8"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YMg5EvHaYn8"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, **kwargs):\n",
        "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQYC3xKeaYn9"
      },
      "outputs": [],
      "source": [
        "line(np.arange(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NrX_4nNaYn9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji18yRwpYlGN"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{DEVICE = }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ouY1sSL6aYn9",
        "outputId": "2a1afe7a-1c6e-4af9-8ad1-9126e8206a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model solu-1l into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "solu_model = HookedTransformer.from_pretrained(\n",
        "    \"solu-1l\",\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True,\n",
        "    device=DEVICE\n",
        ")\n",
        "# gelu_model = HookedTransformer.from_pretrained(\n",
        "#     \"gelu-1l\",\n",
        "#     center_unembed=True,\n",
        "#     center_writing_weights=True,\n",
        "#     fold_ln=True,\n",
        "#     refactor_factored_attn_matrices=True,\n",
        "#     device=DEVICE\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtwEFFTqCb1w"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D2GIV_kwf_iC"
      },
      "outputs": [],
      "source": [
        "# Written by ChatGPT (3) (from Alana's notebook)\n",
        "prompts = [\n",
        "    \"I am happy.\",\n",
        "    \"The sun shines.\",\n",
        "    \"Cats meow.\",\n",
        "    \"Dogs bark.\",\n",
        "    \"Birds fly.\",\n",
        "    \"Time flies.\",\n",
        "    \"Love conquers.\",\n",
        "    \"Dreams inspire.\",\n",
        "    \"Music heals.\",\n",
        "    \"Laughter echoes.\"\n",
        "    \"The sky is blue.\",\n",
        "    \"I love pizza.\",\n",
        "    \"She walks to work.\",\n",
        "    \"He plays guitar.\",\n",
        "    \"We went to the beach.\",\n",
        "    \"They are watching a movie.\",\n",
        "    \"I need some coffee.\",\n",
        "    \"The cat is sleeping.\",\n",
        "    \"He ran to catch the bus.\",\n",
        "    \"She smiled and waved goodbye.\",\n",
        "    \"The book is on the table.\",\n",
        "    \"They laughed at the joke.\",\n",
        "    \"I want to learn coding.\",\n",
        "    \"We had a great time.\",\n",
        "    \"He asked me a question.\",\n",
        "    \"She sings beautifully.\",\n",
        "    \"The sun sets in the west.\",\n",
        "    \"They went hiking in the mountains.\",\n",
        "    \"I forgot my keys at home.\",\n",
        "    \"He bought a new car.\",\n",
        "    \"She enjoys playing tennis.\",\n",
        "    \"We had dinner at a fancy restaurant.\",\n",
        "    \"They are planning a trip to Europe.\",\n",
        "    \"I saw a shooting star last night.\",\n",
        "    \"She wrote a letter to her friend.\",\n",
        "    \"The dog chased its tail.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqn-YcU2CeF3"
      },
      "source": [
        "## Using prompts to compute scores\n",
        "\n",
        "Here we're trying to tackle the [problem 4.39c](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj/p/o6ptPu7arZrqRCxyz#Problems): \"look for tokens where the direct logit attribution of the MLP layer is high, but no single neuron is high.\".\n",
        "\n",
        "- ablation loss - how much ablating that position on that prompt increases loss\n",
        "    - if I'm getting it right, in this particular 1-layer case this should be equivalent to direct logit attribution\n",
        "    - supposed to be a proxy for \"the direct logit attribution of the MLP layer is high\"\n",
        "- inverse outlier score - also per position: `1 / (maximum_activation - mean_activation)`\n",
        "    - supposed to be a proxy for \"no single neuron is high\"\n",
        "\n",
        "So multiplying these two element-wise should give us a reasonable proxy (?; :crossed_fingers:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ccbcf264eb4d46df9241f9c2eb7b12ab"
          ]
        },
        "id": "jubGvF6Xit6y",
        "outputId": "fbe14999-6976-481c-fe6c-5c09499d570b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccbcf264eb4d46df9241f9c2eb7b12ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/35 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from functools import partial\n",
        "import math\n",
        "from pprint import pprint\n",
        "from typing import DefaultDict\n",
        "\n",
        "\n",
        "@dataclass(slots=True)\n",
        "class PromptResults:\n",
        "    \"\"\"To store ~everything (potentially) important for the prompt\"\"\"\n",
        "\n",
        "    index: int\n",
        "    text: str\n",
        "    tokens: torch.Tensor\n",
        "    str_tokens: list[str]\n",
        "\n",
        "    original_loss: float = math.inf\n",
        "\n",
        "    # these are indexed by token position\n",
        "    outlier_scores: torch.Tensor = field(default_factory=lambda: torch.tensor([]))\n",
        "    ablation_losses: torch.Tensor = field(default_factory=lambda: torch.tensor([]))\n",
        "\n",
        "    @classmethod\n",
        "    def make(\n",
        "        cls, model: HookedTransformer, index: int, prompt: str\n",
        "    ) -> \"PromptResults\":\n",
        "        tokens = model.to_tokens(prompt)\n",
        "        str_tokens = model.to_str_tokens(prompt)\n",
        "        original_loss = model(tokens, return_type=\"loss\")\n",
        "        return cls(\n",
        "            index=index,\n",
        "            text=prompt,\n",
        "            tokens=tokens,\n",
        "            str_tokens=str_tokens,\n",
        "            original_loss=original_loss\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def ablation_loss_diffs(self) -> torch.Tensor:\n",
        "        return self.ablation_losses - self.original_loss\n",
        "\n",
        "    @property\n",
        "    def inv_outlier_scores(self) -> torch.Tensor:\n",
        "        return 1 / self.outlier_scores\n",
        "\n",
        "    @property\n",
        "    def length(self) -> int:\n",
        "        return len(self.str_tokens)\n",
        "\n",
        "# indexed by prompt index in the list of prompts\n",
        "prompt_result_dict: dict[int, PromptResults] = {\n",
        "    index: PromptResults.make(solu_model, index, prompt)\n",
        "    for index, prompt in enumerate(prompts)\n",
        "}\n",
        "\n",
        "\n",
        "def compute_outlier_scores(\n",
        "    x: torch.Tensor,\n",
        "    dim: int | None = None,\n",
        "    *,\n",
        "    keepdim: bool = False\n",
        ") -> Float[torch.Tensor, \"batch pos\"]:\n",
        "    return (\n",
        "        x.max(dim=dim, keepdim=keepdim).values\n",
        "        - x.mean(dim=dim, keepdim=keepdim)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def compute_outlier_score_per_pos_hook(\n",
        "    index: int,\n",
        "    x: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "    hook: HookPoint\n",
        ") -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "    outlier_scores = compute_outlier_scores(x, dim=-1)\n",
        "    prompt_result_dict[index].outlier_scores = outlier_scores.squeeze(0)\n",
        "    return x\n",
        "\n",
        "\n",
        "def post_mlp_pre_ln_per_pos_ablation_hook(\n",
        "    pos: int,\n",
        "    x: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "    hook: HookPoint\n",
        ") -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "    x[:, pos, :] = 0\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def compute_ablation_losses(\n",
        "    model: HookedTransformer,\n",
        "    tokens: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    p_len = tokens.size(1)\n",
        "    layer_name = \"blocks.0.mlp.hook_mid\"\n",
        "\n",
        "    ablation_losses = []\n",
        "\n",
        "    for pos in range(p_len):\n",
        "        ablation_hook = partial(post_mlp_pre_ln_per_pos_ablation_hook, pos)\n",
        "        ablation_loss = model.run_with_hooks(\n",
        "            tokens,\n",
        "            return_type=\"loss\",\n",
        "            fwd_hooks=[(layer_name, ablation_hook)]\n",
        "        )\n",
        "        ablation_losses.append(ablation_loss)\n",
        "    return torch.tensor(ablation_losses).to(device=model.cfg.device)\n",
        "\n",
        "\n",
        "for index, pr in tqdm.tqdm(prompt_result_dict.items()):\n",
        "    outlier_score_hook = partial(compute_outlier_score_per_pos_hook, index)\n",
        "    layer_name = \"blocks.0.mlp.hook_mid\"\n",
        "    solu_model.run_with_hooks(\n",
        "        pr.tokens,\n",
        "        fwd_hooks=[(layer_name, outlier_score_hook)]\n",
        "    )\n",
        "    pr.ablation_losses = compute_ablation_losses(solu_model, pr.tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUndryN8JGtW"
      },
      "source": [
        "### Computing scores for important positions with low outlier-ish-ness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6kvBxjGl3YfY"
      },
      "outputs": [],
      "source": [
        "all_ablation_loss_diffs = torch.cat([pr.ablation_loss_diffs for pr in prompt_result_dict.values()], dim=-1)\n",
        "all_inv_outlier_scores = torch.cat([pr.inv_outlier_scores for pr in prompt_result_dict.values()], dim=-1)\n",
        "\n",
        "assert len(all_ablation_loss_diffs) == len(all_inv_outlier_scores) == sum(pr.length for pr in prompt_result_dict.values())\n",
        "\n",
        "scores = all_ablation_loss_diffs * all_inv_outlier_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZGSpvkpJWL1"
      },
      "source": [
        "Parse these scores back into sentences #TODO: explain this better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8_4E7DQC9yFp"
      },
      "outputs": [],
      "source": [
        "def parse_scores(scores: torch.Tensor, prompt_lens: list[int]) -> dict[int, torch.Tensor]:\n",
        "    parsed_scores = {}\n",
        "    parsed_len = 0\n",
        "    for i, prompt_len in enumerate(prompt_lens):\n",
        "        parsed_scores[i] = scores[parsed_len:parsed_len+prompt_len]\n",
        "        parsed_len += prompt_len\n",
        "    return parsed_scores\n",
        "\n",
        "prompt_lens = [pr.length for pr in prompt_result_dict.values()]\n",
        "parsed_scores = parse_scores(scores, prompt_lens)\n",
        "\n",
        "assert sum(len(s) for s in parsed_scores.values()) == len(all_ablation_loss_diffs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5D1IzhLS9Bav"
      },
      "outputs": [],
      "source": [
        "# pprint(parsed_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1TTgS99Kbnn"
      },
      "source": [
        "Now get prompt-position pairs with highest scores, suggesting that they are important positions where \"no single neuron is high\"-ish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "USfJD_7jK9dh",
        "outputId": "6bc88303-9ad4-4443-c15c-d1cc9a21f964"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 18,  38, 152,   8,  23, 151,  48,  33,  29,  70]),\n",
              " tensor([606.1760, 396.3642, 324.9307, 301.1727, 239.6782, 215.7159, 198.5753,\n",
              "         191.8652, 191.6696, 186.2972]),\n",
              " tensor(606.1760))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_top_k_inds(x, k: int = 10) -> list[int]:\n",
        "    return x.sort(descending=True).indices[:k]\n",
        "\n",
        "top_k_inds = get_top_k_inds(scores)\n",
        "top_k_inds, scores[top_k_inds], scores.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5plmpcq9LJfo",
        "outputId": "a57551fd-0bb7-4a73-8f09-20bbd89e8f6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, tensor(3)),\n",
              " (3, tensor(1)),\n",
              " (4, tensor(1)),\n",
              " (5, tensor(2)),\n",
              " (6, tensor(2)),\n",
              " (7, tensor(1)),\n",
              " (9, tensor(1)),\n",
              " (12, tensor(2)),\n",
              " (24, tensor(2)),\n",
              " (24, tensor(1))]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_top_k_from_parsed_scores(parsed_scores: dict[int, torch.Tensor], top_k_inds: torch.Tensor) -> list[tuple[int, int]]:\n",
        "    processed = 0 #TODO: rename\n",
        "    results: list[tuple[int, int]] = []\n",
        "    for index, scores in parsed_scores.items():\n",
        "        length = len(scores)\n",
        "        for i in top_k_inds:\n",
        "            if processed <= i < processed + length:\n",
        "                results.append((index, i - processed))\n",
        "        processed += length\n",
        "    return results\n",
        "\n",
        "top_k = get_top_k_from_parsed_scores(parsed_scores, top_k_inds)\n",
        "top_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lG-a60meLKP5",
        "outputId": "94efdf23-23b1-41a9-ba85-9b6f7a759b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(301.1727)\n",
            "tensor(606.1760)\n",
            "tensor(239.6782)\n",
            "tensor(191.6696)\n",
            "tensor(191.8652)\n",
            "tensor(396.3642)\n",
            "tensor(198.5753)\n",
            "tensor(186.2972)\n",
            "tensor(324.9307)\n",
            "tensor(215.7159)\n"
          ]
        }
      ],
      "source": [
        "for i, pos in top_k:\n",
        "    print(parsed_scores[i][pos])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t1eMM1w5LKoU",
        "outputId": "6ccdf797-bf70-4f02-e2db-36274d953025"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([606.1760, 396.3642, 324.9307, 301.1727, 239.6782, 215.7159, 198.5753,\n",
              "        191.8652, 191.6696, 186.2972])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores[top_k_inds]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqSyabi6S7ve"
      },
      "source": [
        "Yay, works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sLm2RUf6S6sk"
      },
      "outputs": [],
      "source": [
        "def compare_outlier_scores(ind_pos: list[tuple[int, int]]) -> None:\n",
        "    layer_name_pre_ln = \"blocks.0.mlp.hook_mid\"\n",
        "    layer_name_post_ln = \"blocks.0.mlp.ln.hook_normalized\"\n",
        "    for i, pos in ind_pos:\n",
        "        pr = prompt_result_dict[i]\n",
        "        out, cache = solu_model.run_with_cache(pr.tokens)\n",
        "        act_pre = cache[layer_name_pre_ln].squeeze(0)[pos]\n",
        "        act_post = cache[layer_name_post_ln].squeeze(0)[pos]\n",
        "        outlier_score_pre = compute_outlier_scores(act_pre, dim=-1).item()\n",
        "        outlier_score_post = compute_outlier_scores(act_post, dim=-1).item()\n",
        "        post_pre_ratio = outlier_score_post / outlier_score_pre\n",
        "        print(f\"[{i} : {pos}] Pre: {outlier_score_pre:.5f}; Post: {outlier_score_post:.5f}; Ratio: {post_pre_ratio:.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wdM5M0YqLKvx",
        "outputId": "13eb1620-d692-4643-edc2-386d757bf38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Increases in outlier score in allegedly most important positions\n",
            "[1 : 3] Pre: 0.00775; Post: 2.41059; Ratio: 311.22069\n",
            "[3 : 1] Pre: 0.00352; Post: 1.10702; Ratio: 314.65047\n",
            "[4 : 1] Pre: 0.00441; Post: 1.38654; Ratio: 314.24857\n",
            "[5 : 2] Pre: 0.00316; Post: 0.99321; Ratio: 314.14181\n",
            "[6 : 2] Pre: 0.00473; Post: 1.48471; Ratio: 313.65597\n",
            "[7 : 1] Pre: 0.00352; Post: 1.10702; Ratio: 314.65047\n",
            "[9 : 1] Pre: 0.00255; Post: 0.80200; Ratio: 314.41707\n",
            "[12 : 2] Pre: 0.00858; Post: 2.68574; Ratio: 313.03522\n",
            "[24 : 2] Pre: 0.00277; Post: 0.86979; Ratio: 314.53183\n",
            "[24 : 1] Pre: 0.00424; Post: 1.33068; Ratio: 314.00618\n"
          ]
        }
      ],
      "source": [
        "print(\"Increases in outlier score in allegedly most important positions\")\n",
        "compare_outlier_scores(top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nwULbYmCLK8b",
        "outputId": "08b35a88-2afe-4c13-b531-44bd383ecc88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Increases in outlier score in random positions\n",
            "[2 : 1] Pre: 0.00396; Post: 1.24349; Ratio: 314.24359\n",
            "[12 : 0] Pre: 0.03588; Post: 10.66425; Ratio: 297.22357\n",
            "[14 : 2] Pre: 0.00577; Post: 1.80612; Ratio: 313.11740\n",
            "[18 : 5] Pre: 0.00798; Post: 2.49091; Ratio: 312.15110\n",
            "[26 : 6] Pre: 0.00778; Post: 2.42271; Ratio: 311.49457\n",
            "[28 : 3] Pre: 0.01071; Post: 3.34791; Ratio: 312.66995\n",
            "[31 : 4] Pre: 0.00985; Post: 3.08533; Ratio: 313.19182\n",
            "[32 : 6] Pre: 0.02101; Post: 6.51769; Ratio: 310.24370\n",
            "[33 : 2] Pre: 0.00416; Post: 1.30706; Ratio: 314.09935\n",
            "[34 : 2] Pre: 0.00338; Post: 1.06090; Ratio: 313.90099\n"
          ]
        }
      ],
      "source": [
        "rand_inds = random.sample(range(len(scores)), 10)\n",
        "rand_ind_pos = get_top_k_from_parsed_scores(parsed_scores, rand_inds)\n",
        "print(\"Increases in outlier score in random positions\")\n",
        "compare_outlier_scores(rand_ind_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like the ratio is the same for allegedly most important positions (those with greatest ablation loss increase) as for any other."
      ],
      "metadata": {
        "id": "wVuNEsIE9q4w"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb812820b5094695c8a581672e17220e30dd2c15d704c018326e3cc2e1a566f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}